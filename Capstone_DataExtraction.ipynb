{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b6c606",
   "metadata": {},
   "source": [
    "# HRV & T2DM Capstone Project\n",
    "## Data Extraction\n",
    "\n",
    "January-March 2025 | Paul Kalnins, ND (kalninsp@ohsu.edu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aefc62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.20\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5060c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pkalnins/Documents/Capstone/yr2_data/dataset\n",
      "Numpy: 1.24.4\n",
      "Pandas: 2.0.3\n",
      "Matplotlib: 3.7.5\n",
      "Biosspy: 2.1.2\n",
      "WFDB: 4.1.2\n",
      "orjson: 3.10.15\n",
      "Neurokit: 0.2.10\n",
      "Scipy: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import orjson\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "from scipy import signal\n",
    "import biosppy\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# Check current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Test if the libraries are available\n",
    "print(\"Numpy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "print(\"Biosspy:\", biosppy.__version__)\n",
    "print(\"WFDB:\", wfdb.__version__)\n",
    "print(\"orjson:\", orjson.__version__)\n",
    "print(\"Neurokit:\", nk.__version__)\n",
    "print(\"Scipy:\", scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6ca09",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116f89f",
   "metadata": {},
   "source": [
    "### Data Sources \n",
    "\n",
    "- Clinical data\n",
    "    - measurement.csv\n",
    "    - observation.csv\n",
    "    - condition_occurrence.csv\n",
    "    \n",
    "- Wearable Activity Monitory\n",
    "    - heart_rate\n",
    "    - respiratory_rate\n",
    "    - stress\n",
    "    \n",
    "- Cardiac ECG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be8d28",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Extraction of Variables \n",
    "\n",
    "Datasets:\n",
    "\n",
    "- Measurements: \"clinical_data/measurement.csv\"\n",
    "- Conditions: \"clinincal_data/condition_occurrence.csv\"\n",
    "- Observations: \"clinical_data/observation.csv\"\n",
    "\n",
    "The following class `DataExtractor()` has a method `run()` that extracts the relevant data, calculates various metrics, and writes them to the file \"extracted_data/measurements.csv\"\n",
    "\n",
    "Calculations/Metrics Added: \n",
    "\n",
    "- HOMA-IR\n",
    "- HOMA-IR Categories (insulin resistance > 2.5)\n",
    "- Hepatic Steatosis Index (HSI) and categories\n",
    "- Metabolic syndrome (MetS_cat) (0 == no, 1 == yes)\n",
    "- Liver Fat Score (LFS)\n",
    "- LFS category (FLS_cat) (0 == no, 1 == yes)\n",
    "- FIB-4 Score and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8bcbbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    def __init__(self):\n",
    "        self.study_year = 2024\n",
    "        self.output_folder = \"extracted_data\"\n",
    "        self.output_file = \"extracted_data/measurements.csv\"\n",
    "        self.measurements = \"clinical_data/measurement.csv\"\n",
    "        self.observations = \"clinical_data/observation.csv\"\n",
    "        self.conditions = \"clinical_data/condition_occurrence.csv\"\n",
    "        \n",
    "        self.measurement_dict = {\n",
    "            3012888: \"DBP\", 3004249: \"SBP\", 2005200152: \"waist_circum(cm)\", \n",
    "            3036277: \"height(cm)\", 3025315: \"weight(kg)\", 2005200153: \"hip_circum(cm)\", \n",
    "            44809433: \"WHR\", 4245997: \"BMI\", 3004501: \"glucose\", 3004410: \"hba1c\", \n",
    "            3016244: \"insulin\", 3010084: \"c_peptide\", 3010156: \"hs_crp\", \n",
    "            3027114: \"Tchol\", 3022192: \"TG\", 3007070: \"HDL\", 3028288: \"LDL\", \n",
    "            3006923: \"ALT\", 3013721: \"AST\", 3024128: \"bilirubin\", 3035995: \"alk_phos\", \n",
    "            3024561: \"albumin\", 3021886: \"globulin\", 4288601: \"ag_ratio\", \n",
    "            3020630: \"tot_protein\", 3016723: \"creatinine\", 4112223: \"BUN_creatinine\", \n",
    "            3013682: \"BUN\", 3017250: \"creatinine_urine\", 3001802: \"albumin_urine\", \n",
    "            3029187: \"nt_probnp\", 40769783: \"troponin_t\", 3019550: \"sodium\", \n",
    "            3023103: \"potassium\", 3014576: \"chloride\", 3006906: \"calcium\", \n",
    "            3015632: \"CO2\", 2005200183: \"RCB\", 2005200182: \"WCB\", 3007461: \"platelets\", \n",
    "            2005200184: \"hemoglobin\", 3009542: \"hematocrit\", 3024731: \"MCV\", \n",
    "            3035941: \"MCH\", 3003338: \"MCHC\", 3002385: \"RDW\"\n",
    "        }\n",
    "        \n",
    "        self.condition_dict = {\n",
    "            433736: \"obesity\",\n",
    "            37018196: \"pre_DM\",\n",
    "            2005200547: \"elev_hba1c\",\n",
    "            201826: \"T2DM\",\n",
    "            316866: \"HTN\"\n",
    "        }\n",
    "\n",
    "        self.observations_dict = {\n",
    "            4078999: \"brthyy\",\n",
    "            2005200549: \"use_insulin\"\n",
    "        }\n",
    "    \n",
    "    def extract_measurements(self):\n",
    "        \"\"\"\n",
    "        Process clinical data to filter, map, pivot, and save extracted measurements.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "        output_file = os.path.join(self.output_folder, \"measurements.csv\")\n",
    "        \n",
    "        # Extract measurement_concept_ids and measurement names from the dictionary\n",
    "        measurement_concept_ids = list(self.measurement_dict.keys())\n",
    "        measurements = list(self.measurement_dict.values())\n",
    "        \n",
    "        # Load the clinical data\n",
    "        df = pd.read_csv(self.measurements)\n",
    "        \n",
    "        # Filter rows based on measurement_concept_ids and create a copy\n",
    "        filtered_df = df[df[\"measurement_concept_id\"].isin(measurement_concept_ids)].copy()\n",
    "        \n",
    "        # Map measurement_concept_id to measurement_name\n",
    "        filtered_df[\"measurement_name\"] = filtered_df[\"measurement_concept_id\"].map(self.measurement_dict)\n",
    "        \n",
    "        # Pivot the data\n",
    "        pivot_df = filtered_df.pivot_table(\n",
    "            index=\"person_id\", \n",
    "            columns=\"measurement_name\", \n",
    "            values=\"value_as_number\", \n",
    "            aggfunc=\"mean\"\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Round all numeric values to 3 decimal places\n",
    "        pivot_df = pivot_df.round(3)\n",
    "        \n",
    "        # Reorder columns to match the order in the \"measurements\" list\n",
    "        columns_order = [\"person_id\"] + measurements\n",
    "        pivot_df = pivot_df.reindex(columns=columns_order)\n",
    "        \n",
    "        # Save the result to a new CSV file\n",
    "        pivot_df.to_csv(self.output_file, index=False)\n",
    "        print(f\"Extracted measurements: complete\")\n",
    "    \n",
    "    \n",
    "    def extract_conditions(self):\n",
    "        \"\"\"\n",
    "        Process condition data to map conditions to person IDs and merge with measurements.\n",
    "        \"\"\"\n",
    "        measurements = pd.read_csv(self.output_file)\n",
    "        condition_occurrence = pd.read_csv(self.conditions)\n",
    "        \n",
    "        measurements['person_id'] = measurements['person_id'].astype(str)\n",
    "        condition_occurrence['person_id'] = condition_occurrence['person_id'].astype(str)\n",
    "        \n",
    "        # Extract person_id and condition_concept_id from condition_occurrence\n",
    "        condition_occurrence = condition_occurrence[[\"person_id\", \"condition_concept_id\"]]\n",
    "        \n",
    "        # Create an empty dataframe for extracted_conditions with the required structure\n",
    "        extracted_conditions = measurements[[\"person_id\"]].copy()\n",
    "        for condition in self.condition_dict.values():\n",
    "            extracted_conditions[condition] = 0  # Initialize with 0\n",
    "        \n",
    "        # Update extracted_conditions with values from condition_occurrence\n",
    "        for concept_id, condition in self.condition_dict.items():\n",
    "            condition_persons = condition_occurrence.loc[\n",
    "                condition_occurrence[\"condition_concept_id\"] == concept_id, \"person_id\"\n",
    "            ]\n",
    "            extracted_conditions.loc[\n",
    "                extracted_conditions[\"person_id\"].isin(condition_persons), condition\n",
    "            ] = 1\n",
    "        \n",
    "        # Merge the extracted_conditions with the measurements dataframe\n",
    "        merged_data = pd.merge(measurements, extracted_conditions, on=\"person_id\", how=\"left\")\n",
    "        \n",
    "\n",
    "        # Save the merged data back to the measurements file\n",
    "        merged_data.to_csv(self.output_file, index=False)\n",
    "        print(f\"Extracted conditions: complete\")\n",
    "\n",
    "        \n",
    "    def extract_observations(self):\n",
    "        \"\"\"\n",
    "        Process observation data to filter, pivot, and merge with measurements data.\n",
    "        \"\"\"\n",
    "        measurements = pd.read_csv(self.output_file)\n",
    "        observations_data = pd.read_csv(self.observations)\n",
    "        \n",
    "        # Filter the observation data to include only the relevant observation_concept_id's\n",
    "        filtered_observations = observations_data[\n",
    "            observations_data[\"observation_concept_id\"].isin(self.observations_dict.keys())\n",
    "        ]\n",
    "        \n",
    "        # Pivot the filtered data\n",
    "        pivoted_observations = filtered_observations.pivot_table(\n",
    "            index=\"person_id\",\n",
    "            columns=\"observation_concept_id\",\n",
    "            values=\"value_as_number\",\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        \n",
    "        # Rename columns using the observations dictionary\n",
    "        pivoted_observations.rename(columns=self.observations_dict, inplace=True)\n",
    "        \n",
    "        # Merge with the measurements file\n",
    "        extracted_observations = measurements[[\"person_id\"]].merge(\n",
    "            pivoted_observations, on=\"person_id\", how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # Calculate age if \"brthyy\" exists\n",
    "        if \"brthyy\" in extracted_observations.columns:\n",
    "            extracted_observations[\"age\"] = (self.study_year - extracted_observations[\"brthyy\"]).astype(int)\n",
    "\n",
    "            \n",
    "        # Convert \"use_insulin\" to integer\n",
    "        if \"use_insulin\" in extracted_observations.columns:\n",
    "            extracted_observations[\"use_insulin\"] = extracted_observations[\"use_insulin\"].fillna(0).astype(int)\n",
    "        \n",
    "        # Only keep the columns \"use_insulin\" and \"age\"\n",
    "        extracted_observations = extracted_observations[[\"person_id\", \"use_insulin\", \"age\"]]\n",
    "        \n",
    "        # Merge the extracted observations with the measurements data\n",
    "        merged_data = measurements.merge(extracted_observations, on=\"person_id\", how=\"left\")\n",
    "        \n",
    "        # Save the resulting dataframe back to the measurements file\n",
    "        merged_data.to_csv(self.output_file, index=False)\n",
    "        print(f\"Extracted observations: complete\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run all steps of the data extraction and merging process.\n",
    "        \"\"\"\n",
    "        self.extract_measurements()\n",
    "        self.extract_conditions()\n",
    "        self.extract_observations()\n",
    "        print(\"All metrics added to 'extracted_data/measurements.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f4b5e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted measurements: complete\n",
      "Extracted conditions: complete\n",
      "Extracted observations: complete\n",
      "All metrics added to 'extracted_data/measurements.csv'\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Object from the Class and run\n",
    "# The following dictionaries are variables extracted from each dataset\n",
    "\n",
    "extractor = DataExtractor()\n",
    "extractor.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b623d7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Extract Garmin Watch Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0044d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class WearableDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.base_dir = \"wearable_activity_monitor\"\n",
    "        self.manifest_file = \"wearable_activity_monitor/manifest.tsv\"\n",
    "        self.out_file = \"extracted_data/measurements.csv\"\n",
    "\n",
    "    def load_json_data(self, filepath):\n",
    "        if not os.path.isfile(filepath):\n",
    "            return None\n",
    "        with open(filepath, 'rb') as file:\n",
    "            return orjson.loads(file.read())\n",
    "    \n",
    "    def safe_filepath(self, value):\n",
    "        return \"\" if pd.isna(value) or value in [None, \"None\", \"none\", \"\"] else value.lstrip('/')\n",
    "\n",
    "\n",
    "    def process_stress_json(self, data):\n",
    "        stress_values = []\n",
    "        stress_dates = set()  # To store unique dates\n",
    "\n",
    "        for entry in data[\"body\"][\"stress\"]:\n",
    "            stress_value = entry[\"stress\"][\"value\"]\n",
    "            if stress_value > 0:\n",
    "                stress_values.append(stress_value)\n",
    "                date = entry[\"effective_time_frame\"][\"date_time\"][:10]  # Extract date (yyyy-mm-dd)\n",
    "                stress_dates.add(date)\n",
    "\n",
    "        if len(stress_dates) > 2:  # Check if there are more than 2 unique days\n",
    "            return round(sum(stress_values) / len(stress_values), 2) if stress_values else None\n",
    "        else:\n",
    "            return None  # Return NA if there are fewer than 2 days\n",
    "\n",
    "\n",
    "    def process_heart_rate(self, data):\n",
    "        hr_rates = []\n",
    "        hr_dates = set()\n",
    "\n",
    "        for entry in data.get(\"body\", {}).get(\"heart_rate\", []):\n",
    "            hr_rate = entry[\"heart_rate\"][\"value\"]\n",
    "            if hr_rate > 0:\n",
    "                hr_rates.append(hr_rate)\n",
    "                date = entry[\"effective_time_frame\"][\"date_time\"][:10]  # Extract date (yyyy-mm-dd)\n",
    "                hr_dates.add(date)\n",
    "\n",
    "        if len(hr_dates) > 2:  # Only compute average if there are more than 2 unique days\n",
    "            return round(np.mean(hr_rates), 2) if hr_rates else None\n",
    "        else:\n",
    "            return None  # Return NA if fewer than 2 days of data\n",
    "\n",
    "\n",
    "    def process_respiratory_rate(self, data):\n",
    "        rr_values = []\n",
    "        rr_dates = set()  # To store unique dates\n",
    "\n",
    "        for entry in data.get(\"body\", {}).get(\"breathing\", []):\n",
    "            rr_value = entry[\"respiratory_rate\"][\"value\"]\n",
    "            if rr_value > 0:  # Only consider valid respiratory rates\n",
    "                rr_values.append(rr_value)\n",
    "                date = entry[\"effective_time_frame\"][\"date_time\"][:10]  # Extract date (yyyy-mm-dd)\n",
    "                rr_dates.add(date)\n",
    "\n",
    "        if len(rr_dates) > 2:  # Only compute average if there are more than 2 unique days\n",
    "            return round(np.mean(rr_values), 2) if rr_values else None\n",
    "        else:\n",
    "            return None  # Return NA if fewer than 2 days of data\n",
    "\n",
    "\n",
    "    def calculate_prq(self, avg_hr, avg_rr):\n",
    "        if not avg_hr or not avg_rr:\n",
    "            return None\n",
    "        return round((avg_hr / avg_rr), 2)\n",
    "    \n",
    "    def get_sleep_metrics(self, data):\n",
    "        # Initialize dictionary to track the duration of each sleep stage\n",
    "        sleep_stages = {\"light\": 0, \"deep\": 0, \"awake\": 0, \"rem\": 0}\n",
    "        sleep_dates = set()  # To store unique dates\n",
    "\n",
    "        for entry in data.get(\"body\", {}).get(\"sleep\", []):\n",
    "            sleep_stage = entry[\"sleep_stage_state\"]\n",
    "\n",
    "            # Check if the sleep_stage is one of the recognized stages\n",
    "            if sleep_stage not in sleep_stages:\n",
    "                continue  # Skip any unknown sleep stages\n",
    "\n",
    "            start_time = entry[\"sleep_stage_time_frame\"][\"time_interval\"][\"start_date_time\"]\n",
    "            end_time = entry[\"sleep_stage_time_frame\"][\"time_interval\"][\"end_date_time\"]\n",
    "\n",
    "            # Calculate the duration of each sleep stage (in minutes)\n",
    "            start_dt = datetime.fromisoformat(start_time[:-1])  # Convert from ISO format\n",
    "            end_dt = datetime.fromisoformat(end_time[:-1])\n",
    "            duration = (end_dt - start_dt).total_seconds() / 60  # Duration in minutes\n",
    "\n",
    "            sleep_stages[sleep_stage] += duration\n",
    "            date = start_time[:10]  # Extract date (yyyy-mm-dd)\n",
    "            sleep_dates.add(date)\n",
    "\n",
    "        if len(sleep_dates) > 2:  # Only compute averages if there are more than 2 unique days\n",
    "            # Total sleep time is the sum of \"light\", \"deep\", and \"rem\" stages\n",
    "            total_sleep_time = sleep_stages[\"light\"] + sleep_stages[\"deep\"] + sleep_stages[\"rem\"]\n",
    "            # Total time in bed is the sum of \"light\", \"deep\", \"awake\", and \"rem\" stages\n",
    "            total_time_in_bed = sleep_stages[\"light\"] + sleep_stages[\"deep\"] + sleep_stages[\"awake\"] + sleep_stages[\"rem\"]\n",
    "\n",
    "            # Calculate Sleep Efficiency Ratio (SER)\n",
    "            ser = (total_sleep_time / total_time_in_bed) * 100 if total_time_in_bed > 0 else None\n",
    "            return round(ser, 2)  # Return the Sleep Efficiency Ratio as a percentage\n",
    "        else:\n",
    "            return None  # Return NA if fewer than 2 days of data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def process_participant(self, row): \n",
    "        person_id = row[\"participant_id\"]\n",
    "        stress_file = self.safe_filepath(row[\"stress_level_filepath\"])\n",
    "        hr_file = self.safe_filepath(row[\"heartrate_filepath\"])\n",
    "        rr_file = self.safe_filepath(row[\"respiratory_rate_filepath\"])\n",
    "        sleep_file = self.safe_filepath(row[\"sleep_filepath\"])\n",
    "\n",
    "        stress_data = self.load_json_data(stress_file) if stress_file else None\n",
    "        hr_data = self.load_json_data(hr_file) if hr_file else None\n",
    "        rr_data = self.load_json_data(rr_file) if rr_file else None\n",
    "        sleep_data = self.load_json_data(sleep_file) if sleep_file else None\n",
    "\n",
    "        avg_stress = self.process_stress_json(stress_data) if stress_data else None\n",
    "        avg_hr = self.process_heart_rate(hr_data) if hr_data else None\n",
    "        avg_rr = self.process_respiratory_rate(rr_data) if rr_data else None\n",
    "        avg_prq = self.calculate_prq(avg_hr, avg_rr)\n",
    "\n",
    "        ser = self.get_sleep_metrics(sleep_data) if sleep_data else None\n",
    "\n",
    "        # Updated metrics to only include SER\n",
    "        metrics = {\n",
    "            \"stress_gv5\": avg_stress,\n",
    "            \"HR_gv5\": avg_hr,\n",
    "            \"RR_gv5\": avg_rr,\n",
    "            \"PRQ_gv5\": avg_prq,\n",
    "            \"SER\": ser,  \n",
    "        }\n",
    "        return person_id, metrics\n",
    "\n",
    "\n",
    "        \n",
    "    def process_wearable_data(self):\n",
    "        # Load the existing measurements file\n",
    "        measurements_file = \"extracted_data/measurements.csv\"\n",
    "        measurements_df = pd.read_csv(measurements_file)\n",
    "\n",
    "        # Load the manifest file\n",
    "        manifest = pd.read_csv(self.manifest_file, sep='\\t')\n",
    "\n",
    "        # Convert manifest to list of dictionaries for parallel processing\n",
    "        manifest_records = manifest.to_dict(\"records\")\n",
    "\n",
    "        # Process participants in parallel with a progress bar\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(self.process_participant, manifest_records), total=len(manifest_records), desc=\"Processing Wearable Data\"))\n",
    "\n",
    "        # Convert results list to DataFrame\n",
    "        results_dict = {person_id: metrics for person_id, metrics in results}  # Convert list of tuples to dict\n",
    "        new_data_df = pd.DataFrame.from_dict(results_dict, orient=\"index\").reset_index()\n",
    "        new_data_df.rename(columns={\"index\": \"person_id\"}, inplace=True)\n",
    "\n",
    "        # Merge with the existing measurements file on 'person_id'\n",
    "        updated_df = measurements_df.merge(new_data_df, on=\"person_id\", how=\"left\")\n",
    "\n",
    "        # Save the updated file\n",
    "        updated_df.to_csv(measurements_file, index=False)\n",
    "        print(f\"Output saved to {measurements_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960adce",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### HRV Metrics Extraction\n",
    "\n",
    "Metrics of interest: \n",
    "\n",
    "- Time-domain: SDNN, RMSSD\n",
    "- Frequency-domain: LF, HF, LF/HF ratio\n",
    "- Non-linear: sd_1, sd_2, sd_1/sd_2 ratio\n",
    "\n",
    "Practical Metrics to Gather (due to short ~11 sec ECG readings):\n",
    "\n",
    "- Time-domain: SDNN, RMSSD\n",
    "- Also: ln(RMSSD normalized to heart rate)\n",
    "\n",
    "Steps: \n",
    "\n",
    "- Install needed libraries (wfdb, biosppy, pyhrv, numpy, matplotlib)\n",
    "- Load WFDB files using wfdb\n",
    "- Extract RR intervals using biosppy\n",
    "- Calculate HRV metrics using pyhrv (time-domain, frequency-domain, non-linear)\n",
    "- Visualize and save results \n",
    "\n",
    "Notes: \n",
    "\n",
    "- ECG samples are only 11 seconds long, with some artifact at the end; I compared RR-intervals from heartpy, neurokit2, and biosppy.  Only biosppy correctly removes the artifact.\n",
    "- Since the ECG samples are only 11 seconds long, I will only use SDNN and RMSSD as metrics, as these have some meaning for such short time intervals.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "64d6a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRVProcessor:\n",
    "    def __init__(self):\n",
    "        self.lead = 10\n",
    "        self.trim_duration = 0.5\n",
    "        self.manifest_file = \"cardiac_ecg/manifest.tsv\"\n",
    "        self.measurements_file = \"extracted_data/measurements.csv\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_or_none(value): \n",
    "        return round(value, 2) if value is not None else None\n",
    "    \n",
    "    def extract_hrv_nk(self, ecg_file):\n",
    "        # Load ECG record\n",
    "        record = wfdb.rdrecord(ecg_file)\n",
    "        ecg_signal = record.p_signal[:, self.lead]  # Select specified lead\n",
    "        fs = record.fs  # Sampling frequency\n",
    "\n",
    "        # Trim and clean the signal to remove artifacts\n",
    "        samples_to_trim = int(self.trim_duration * fs)\n",
    "        ecg_signal_trimmed = ecg_signal[samples_to_trim:-samples_to_trim]\n",
    "        ecg_cleaned = nk.ecg_clean(ecg_signal_trimmed, sampling_rate=fs)\n",
    "\n",
    "        # Detect & extract R-peaks using NeuroKit2 | convert to milliseconds\n",
    "        rpeaks, _ = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs)\n",
    "        rpeaks_samples = np.where(rpeaks['ECG_R_Peaks'])[0]\n",
    "        rpeaks_ms = [idx * 1000 / fs for idx in rpeaks_samples]\n",
    "\n",
    "        # Compute NN intervals (in milliseconds)\n",
    "        nni = np.diff(rpeaks_ms)\n",
    "\n",
    "        if len(nni) > 1:\n",
    "            # Calculate HRV metrics\n",
    "            sdnn = np.std(nni, ddof=1)\n",
    "            diff_nni = np.diff(nni)\n",
    "            rmssd = np.sqrt(np.mean(diff_nni ** 2)) if len(diff_nni) > 0 else None\n",
    "            \n",
    "            # Calculate Heart Rate (HR)\n",
    "            mean_nni = np.mean(nni)\n",
    "            hr_ecg = 60000/mean_nni if mean_nni > 0 else None\n",
    "            \n",
    "            # Compute RMSSD normalized to HR, and ln(RMSSD_norm)\n",
    "            rmssd_norm = rmssd / hr_ecg if rmssd and hr_ecg else None\n",
    "            rmssd_norm_ln = np.log(rmssd_norm) if rmssd_norm and rmssd_norm > 0 else None\n",
    "\n",
    "            # Compute SDNN normalized to HR, and ln(SDNN_norm)\n",
    "            sdnn_norm = sdnn / hr_ecg if sdnn and hr_ecg else None\n",
    "            sdnn_norm_ln = np.log(sdnn_norm) if sdnn_norm and sdnn_norm > 0 else None\n",
    "            \n",
    "        else:\n",
    "            sdnn = rmssd = hr_ecg = rmssd_norm = rmssd_norm_ln = sdnn_norm = sdnn_norm_ln = None\n",
    "\n",
    "        return {\n",
    "            'sdnn': self.round_or_none(sdnn),\n",
    "            'rmssd': self.round_or_none(rmssd), \n",
    "            'hr_ecg': self.round_or_none(hr_ecg), \n",
    "            'cSDNN': self.round_or_none(sdnn_norm_ln),  \n",
    "            'cRMSSD': self.round_or_none(rmssd_norm_ln),       \n",
    "        }\n",
    "    \n",
    "    def process_participant(self, row): \n",
    "        index, row = row\n",
    "        participant_id = row.get('participant_id')\n",
    "        hr_ecg_manifest = row.get('Rate', np.nan)\n",
    "        base_filepath = os.path.splitext(row['wfdb_hea_filepath'])[0].lstrip('/')\n",
    "        \n",
    "        try: \n",
    "            hrv_metrics = self.extract_hrv_nk(base_filepath)\n",
    "            if hrv_metrics: \n",
    "                hrv_metrics['person_id'] = participant_id\n",
    "                return hrv_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {base_filepath}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_hrv_metrics(self):\n",
    "        manifest_df = pd.read_csv(self.manifest_file, sep='\\t') # Load manifest file\n",
    "        all_hrv_metrics = []\n",
    "\n",
    "        # Create a progress bar and ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(self.process_participant, manifest_df.iterrows()), total=len(manifest_df), desc=\"Processing HRV metrics\"))\n",
    "\n",
    "        # Filter out None results (in case of errors) and convert to DataFrame\n",
    "        all_hrv_metrics = [result for result in results if result is not None]\n",
    "        hrv_df = pd.DataFrame(all_hrv_metrics)\n",
    "\n",
    "        if not hrv_df.empty:\n",
    "            hrv_df.set_index('person_id', inplace=True)\n",
    "            existing_df = pd.read_csv(self.measurements_file, index_col='person_id')\n",
    "            updated_df = existing_df.merge(hrv_df, left_index=True, right_index=True, how=\"left\")\n",
    "            updated_df.to_csv(self.measurements_file, index=True)\n",
    "            print(f\"ECG metrics saved to {self.measurements_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0030d6",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Function to Run all Data Extraction Sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6ff1af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \n",
    "    # Step 1: Extract measurements, conditions, and observations\n",
    "    extractor = DataExtractor()\n",
    "    extractor.run()\n",
    "    \n",
    "    # Step 2: Process wearable data\n",
    "    processor = WearableDataProcessor()\n",
    "    processor.process_wearable_data()\n",
    "    \n",
    "    # Step 3: Process HRV metrics\n",
    "    hrv_processor = HRVProcessor()\n",
    "    hrv_processor.process_hrv_metrics()\n",
    "\n",
    "    print(\"Data processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ec801265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted measurements: complete\n",
      "Extracted conditions: complete\n",
      "Extracted observations: complete\n",
      "All metrics added to 'extracted_data/measurements.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wearable Data: 100%|███████████████| 905/905 [50:37<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to extracted_data/measurements.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HRV metrics: 100%|███████████████| 1059/1059 [06:09<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG metrics saved to extracted_data/measurements.csv\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Run this function to process all data\n",
    "process_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558b7a2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Function to Extract Final Variables, Filter, and Save to Master File for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b2e5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFinalData(): \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_file = \"extracted_data/measurements.csv\"\n",
    "        self.output_file = \"extracted_data/final_data.csv\"\n",
    "    \n",
    "        self.columns_to_keep = [\n",
    "            \"age\", \"BMI\", \"WHR\", \"waist_circum(cm)\",\n",
    "            \"SBP\", \"DBP\", \n",
    "            \"glucose\", \"hba1c\", \"insulin\", \"hs_crp\", \n",
    "            \"Tchol\", \"TG\", \"HDL\", \"LDL\", \"bilirubin\", \"alk_phos\", \n",
    "            \"nt_probnp\", \"troponin_t\",  \n",
    "            \"BUN\", \"creatinine\", \n",
    "            \"obesity\", \"pre_DM\", \"elev_hba1c\", \"T2DM\", \"use_insulin\", \"HTN\", \n",
    "            \"stress_gv5\", \"HR_gv5\", \"RR_gv5\", \"PRQ_gv5\", \"SER\",  \n",
    "            \"sdnn\", \"rmssd\", \"cRMSSD\", \"cSDNN\", \"hr_ecg\",\n",
    "            \"MetS_cat\", \"SAFE\", \"SAFE_cat\"\n",
    "        ]\n",
    "    \n",
    "    def load_and_preprocess(self): \n",
    "        df = pd.read_csv(self.input_file)\n",
    "        \n",
    "        cols_to_numeric = [\n",
    "            \"DBP\", \"SBP\", \"WHR\", \"BMI\", \n",
    "            \"glucose\", \"hba1c\", \"insulin\", \"hs_crp\",  \n",
    "            \"Tchol\", \"TG\", \"HDL\", \"LDL\", \n",
    "            \"BUN\", \"creatinine\", \"age\",\n",
    "            \"stress_gv5\", \"HR_gv5\", \"RR_gv5\", \"PRQ_gv5\", \"SER\",  \n",
    "            \"sdnn\", \"rmssd\", \"cRMSSD\", \"cSDNN\", \"hr_ecg\"\n",
    "        ]\n",
    "        for col in cols_to_numeric:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')  \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_metabolic_syndrome(self, df):\n",
    "        df['MetS_cat'] = (\n",
    "            (df['waist_circum(cm)'] >= 95).astype(int) +\n",
    "            (df['TG'] >= 150).astype(int) +\n",
    "            (df['HDL'] < 45).astype(int) +\n",
    "            (df['SBP'] >= 130).astype(int) +\n",
    "            (df['glucose'] >= 100).astype(int)\n",
    "        ) >= 3\n",
    "        df['MetS_cat'] = df['MetS_cat'].astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_fibrosis_metrics(self, df):\n",
    "        log_vars = ['AST', 'ALT', 'globulin', 'platelets']\n",
    "        \n",
    "        for var in log_vars:\n",
    "            df[f'Ln_{var}'] = np.log(df[var].replace(0, 0.0001))\n",
    "            \n",
    "        # Create the 'DM_lab' column based on HbA1c\n",
    "        #df['DM_lab'] = np.where(df['hba1c'] >= 6.5, 1, 0) \n",
    "        \n",
    "        # SAFE Score\n",
    "        df['BMI_capped'] = df['BMI'].clip(upper=40)\n",
    "        df['SAFE'] = (\n",
    "            (2.97 * df['age']) +\n",
    "            (5.99 * df['BMI_capped']) +\n",
    "            (62.85 * df['T2DM']) +\n",
    "            (154.85 * df['Ln_AST']) -\n",
    "            (58.23 * df['Ln_ALT']) +\n",
    "            (195.48 * df['Ln_globulin']) -\n",
    "            (141.61 * df['Ln_platelets']) - 75\n",
    "        )\n",
    "        df['SAFE_cat'] = np.nan\n",
    "        df.loc[df['SAFE'] < 0.0, 'SAFE_cat'] = 0\n",
    "        df.loc[df['SAFE'] >= 0.00, 'SAFE_cat'] = 1\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_and_filter_data(self, df):\n",
    "        # Replace 0 values with nan\n",
    "        df['SER'] = df['SER'].replace(0, np.nan)\n",
    "        df['hba1c'] = df['hba1c'].replace(0, np.nan)\n",
    "        \n",
    "        # Replace values > 10 with nan\n",
    "        #df['hs_crp'] = df['hs_crp'].apply(lambda x: np.nan if x > 10 else x)\n",
    "        \n",
    "        # Apply log transformation to hs_crp\n",
    "        #df['ln_hs_crp'] = np.where(df['hs_crp'] > 0, np.log(df['hs_crp']), np.nan)\n",
    "\n",
    "        # Round selected metrics\n",
    "        df = df.round({'SAFE': 2, 'TyG_index': 2})\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        df = self.load_and_preprocess()\n",
    "        df = self.calculate_metabolic_syndrome(df)\n",
    "        df = self.calculate_fibrosis_metrics(df)\n",
    "        df = self.clean_and_filter_data(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def filter_save_data(self):\n",
    "        df = self.calculate_metrics()\n",
    "        \n",
    "        num_obs_before = len(df)\n",
    "        print(f\"Total observations before filtering: {num_obs_before}\")\n",
    "        \n",
    "        #df_filtered = df[df['stress_gv5'].notna()] # Remove rows without sensor data\n",
    "        df_filtered = df[[\"person_id\"] + self.columns_to_keep] # Keep only selected columns\n",
    "        df_filtered.set_index(\"person_id\", inplace=True)\n",
    "        df_filtered.to_csv(self.output_file)\n",
    "        \n",
    "        num_obs_after = len(df_filtered)\n",
    "        print(f\"Filtered dataset saved to {self.output_file}\")\n",
    "        print(f\"Total observations after filtering: {num_obs_after}\")\n",
    "        print(f\"Participants lacking sensor data: {num_obs_before - num_obs_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18df516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \n",
    "    # Step 1: Extract measurements, conditions, and observations\n",
    "    extractor = DataExtractor()\n",
    "    extractor.run()\n",
    "    \n",
    "    # Step 2: Process wearable data\n",
    "    processor = WearableDataProcessor()\n",
    "    processor.process_wearable_data()\n",
    "    \n",
    "    # Step 3: Process HRV metrics\n",
    "    hrv_processor = HRVProcessor()\n",
    "    hrv_processor.process_hrv_metrics()\n",
    "    \n",
    "    # Step 4: Create metrics and filter data\n",
    "    filtering = CreateFinalData()\n",
    "    filtering.filter_save_data()\n",
    "    \n",
    "    print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8436c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted measurements: complete\n",
      "Extracted conditions: complete\n",
      "Extracted observations: complete\n",
      "All metrics added to 'extracted_data/measurements.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wearable Data: 100%|███████████████| 905/905 [21:57<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to extracted_data/measurements.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HRV metrics: 100%|███████████████| 1059/1059 [03:03<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG metrics saved to extracted_data/measurements.csv\n",
      "Filtered dataset saved to extracted_data/filtered_data.csv\n",
      "Total observations after filtering: 469\n",
      "Data processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ab8f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total observations before filtering: 1077\n",
      "Filtered dataset saved to extracted_data/final_data.csv\n",
      "Total observations after filtering: 1077\n",
      "Participants lacking sensor data: 0\n"
     ]
    }
   ],
   "source": [
    "# Run this function to extract final variables, filter, and save them to file\n",
    "\n",
    "filtering = CreateFinalData()\n",
    "filtering.filter_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037a1bb-49db-4a72-8300-37247bd4f6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b734b2e-deb4-4585-bd05-d9ad5948d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of participants: 1077\n",
      "\n",
      "Rows removed due to sndd > 150 ms: 71\n",
      "\n",
      "Observations per column:\n",
      "                  Original_total  Missing  Filtered_total  Filtered_missing\n",
      "person_id                   1077        0             740                 0\n",
      "age                         1077        0             740                 0\n",
      "BMI                         1075        2             740                 0\n",
      "WHR                         1072        5             740                 0\n",
      "waist_circum(cm)            1074        3             740                 0\n",
      "SBP                         1077        0             740                 0\n",
      "DBP                         1077        0             740                 0\n",
      "glucose                     1032       45             740                 0\n",
      "hba1c                       1022       55             734                 6\n",
      "insulin                     1030       47             738                 2\n",
      "hs_crp                      1032       45             740                 0\n",
      "Tchol                       1032       45             740                 0\n",
      "TG                          1032       45             740                 0\n",
      "HDL                         1032       45             740                 0\n",
      "LDL                         1032       45             740                 0\n",
      "bilirubin                   1032       45             740                 0\n",
      "alk_phos                    1032       45             740                 0\n",
      "nt_probnp                   1032       45             740                 0\n",
      "troponin_t                  1032       45             740                 0\n",
      "BUN                         1032       45             740                 0\n",
      "creatinine                  1032       45             740                 0\n",
      "obesity                     1077        0             740                 0\n",
      "pre_DM                      1077        0             740                 0\n",
      "elev_hba1c                  1077        0             740                 0\n",
      "T2DM                        1077        0             740                 0\n",
      "use_insulin                 1077        0             740                 0\n",
      "HTN                         1077        0             740                 0\n",
      "stress_gv5                   900      177             740                 0\n",
      "HR_gv5                       904      173             740                 0\n",
      "RR_gv5                       897      180             740                 0\n",
      "PRQ_gv5                      895      182             740                 0\n",
      "SER                          873      204             740                 0\n",
      "sdnn                        1057       20             740                 0\n",
      "rmssd                       1057       20             740                 0\n",
      "cRMSSD                      1057       20             740                 0\n",
      "cSDNN                       1057       20             740                 0\n",
      "hr_ecg                      1057       20             740                 0\n",
      "MetS_cat                    1077        0             740                 0\n",
      "SAFE                        1027       50             740                 0\n",
      "SAFE_cat                    1027       50             740                 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"extracted_data/final_data.csv\")\n",
    "print(\"Total number of participants:\", len(df))\n",
    "\n",
    "columns_to_check = ['WHR', 'waist_circum(cm)', 'stress_gv5', 'PRQ_gv5', 'SER', 'sdnn', 'rmssd', 'SAFE']\n",
    "filtered_df = df.dropna(subset=columns_to_check)\n",
    "\n",
    "# Filter HRV values\n",
    "before_sdnn_filter = len(filtered_df)\n",
    "hrv_threshold = 100\n",
    "filtered_df = filtered_df[(filtered_df['sdnn'] <= hrv_threshold) & (filtered_df['rmssd'] <= hrv_threshold)]\n",
    "\n",
    "removed_due_to_sdnn = before_sdnn_filter - len(filtered_df)\n",
    "print(f\"\\nRows removed due to sndd > 150 ms: {removed_due_to_sdnn}\")\n",
    "\n",
    "filtered_df.to_csv(\"extracted_data/filtered_data.csv\", index=False)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Original_total\": df.count(), \n",
    "    \"Missing\": df.isnull().sum(), \n",
    "    \"Filtered_total\": filtered_df.count(), \n",
    "    \"Filtered_missing\": filtered_df.isnull().sum()\n",
    "})\n",
    "\n",
    "print(\"\\nObservations per column:\")\n",
    "print(summary)\n",
    "\n",
    "summary.to_csv(\"filtered_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729f6a6-ffe3-49d6-98bd-09557ed19716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
